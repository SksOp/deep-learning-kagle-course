{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# #set matplotlib defaults\n",
    "# plt.rc('figure' , autolayout = True)\n",
    "# plt.rc('axes' , labelweight = 'bold' , labelsize = 'large' , titleweight = 'bold' , titlesize = 18 , titlepad = 10)\n",
    "# plt.rc('animation' , html = 'html5')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "spotify = pd.read_csv('./archive/spotify.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = spotify.isna().sum()\n",
    "# print(nan_counts)\n",
    "X = spotify.copy().dropna()  #drop rows with missing target \n",
    "y = X.pop('track_popularity')\n",
    "artists = X['track_artist']\n",
    "\n",
    "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
    "                'speechiness', 'acousticness', 'instrumentalness',\n",
    "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "\n",
    "features_cat = ['playlist_genre']\n",
    "\n",
    "# #puting features num in data frame to visualize\n",
    "# X_num = X[features_num].copy()\n",
    "# X_num.head()\n",
    "\n",
    "#printing the total number of unique artists\n",
    "print(f'There are {len(artists.unique())} artists in the dataset.')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll do a \"grouped\" split to keep all of an artist's songs in one\n",
    "# split or the other. This is to help prevent signal leakage.\n",
    "def group_split (X,y,group,train_size = 0.75):\n",
    "    splitter = GroupShuffleSplit(train_size = train_size)\n",
    "    train,test = next(splitter.split(X,y,groups = group))\n",
    "    return (X.iloc[train],X.iloc[test],y.iloc[train],y.iloc[test])\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = group_split(X,y,artists)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "y_train = y_train / 100\n",
    "y_valid = y_valid / 100\n",
    "\n",
    "# df = pd.DataFrame(X_train)\n",
    "# display(df.head())\n",
    "# df = pd.DataFrame(y_train)\n",
    "# display(df.head())\n",
    "\n",
    "input_shape = [X_train.shape[1]]\n",
    "print('Input shape: {}'.format(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1,input_shape = input_shape),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'mae',)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data = (X_valid,y_valid),\n",
    "    batch_size = 512,\n",
    "    epochs = 50,\n",
    "    verbose = 0,\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:,['loss','val_loss']].plot()\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128,activation = 'relu',input_shape = input_shape),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(1)])\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'mae',)\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data = (X_valid,y_valid),\n",
    "    batch_size = 512,\n",
    "    epochs = 50,\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:,['loss','val_loss']].plot()\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping\n",
    "from tensorflow.keras import callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 5,\n",
    "    restore_best_weights = True,\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128,activation = 'relu',input_shape = input_shape),\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(1)])\n",
    "model.compile(\n",
    "    loss=\"mae\",\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data=(X_valid,y_valid),\n",
    "    callbacks=early_stopping,\n",
    "    epochs=50,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:,[\"loss\",\"val_loss\"]].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128,activation = 'relu',input_shape = input_shape),\n",
    "    layers.Dropout(rate=0.5) , # apply 30% dropuot to next layer\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dense(1)])\n",
    "model.compile(\n",
    "    loss=\"mae\",\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data=(X_valid,y_valid),\n",
    "    # callbacks=early_stopping,\n",
    "    epochs=50,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:,[\"loss\",\"val_loss\"]].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128,activation = 'relu',input_shape = input_shape),\n",
    "    layers.Dropout(rate=0.5) , # apply 30% dropuot to next layer\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.Dropout(rate=0.5) , # apply 30% dropuot to next layer\n",
    "    layers.Dense(1)])\n",
    "model.compile(\n",
    "    loss=\"mae\",\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data=(X_valid,y_valid),\n",
    "    # callbacks=early_stopping,\n",
    "    epochs=50,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:,[\"loss\",\"val_loss\"]].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dropout(rate=0.5) , # apply 30% dropuot to next layer\n",
    "    layers.Dense(128,activation = 'relu',input_shape = input_shape),\n",
    "    layers.Dropout(rate=0.5) , # apply 30% dropuot to next layer\n",
    "    layers.Dense(64,activation = 'relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1)])\n",
    "model.compile(\n",
    "    loss=\"mae\",\n",
    "    optimizer=\"sgd\"\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data=(X_valid,y_valid),\n",
    "    # callbacks=early_stopping,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:,[\"loss\",\"val_loss\"]].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
